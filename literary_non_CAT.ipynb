{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8143bf8-9339-4d16-a91d-3cbb48751df8",
   "metadata": {},
   "source": [
    "Jana Bruses janabruses@pitt.edu Jan. 14 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c7bf96-c81e-4780-91eb-f1c8247efa54",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "**Name:** CORPUS TEXTUAL INFORMATITZAT DE LA LLENGUA CATALANA (CTILC) \"Informatized textual corpora of the Catalan language\"\n",
    "\n",
    "**Author:** Institut d'Estudis Catalans (IEC) \"Institute of Catalan studies.\"\n",
    "\n",
    "**Format:** raw text files (.txt)\n",
    "\n",
    "**About it:**\\\n",
    "Project started in 1985 as a corpora containing texts published between 1832 and 1988.\n",
    "Starting in 2015, texts published after 1985 were included to breach the time gap. \n",
    "The initial goal of this corpora was to be a source of data for the development of the descriptive dictionary of the Catalan language known as DDLC.\\\n",
    "Nowadays, it has been made into a consultation page, allowing us to search concordances, colocations, and numerical data online.\\\n",
    "Part of the corpora has been made available for public use. Only those works that are no longer subject to copyright in Spain are being made public work by work in single text files.\n",
    "Hence, the following work is only on the available documents, and therefore, older corpora. Consequently, the analysis is subject to its time.\\\n",
    "The downloadable corpora consists of 337 files of literary works and 596 non-literary texts.\\\n",
    "Because of copyright reasons, the files can only be downloaded one by one. Consequently, in this analysis, only the files written by authors whose names started with the letter \"A\" were used.\n",
    "\n",
    "**Licence:**\\\n",
    "This corpora is completely public and can be downloaded for private use or for research use with the license Creative Commons.\\ \n",
    "They ask any user that makes use of any of the texts to cite its origin with the following text in Catalan: \"Aquest text ha estat digitalitzat i processat per l'Institut d'Estudis Catalans, com a part del projecte Corpus Textual Informatitzat de la Llengua Catalana\".\n",
    "\n",
    "**URL:** https://ctilc.iec.cat/scripts/CTILCCorpus_Descarr.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5507fa30-e1c1-4e60-bde1-befac674ef21",
   "metadata": {},
   "source": [
    "## Summary and discovery question:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394fdcf-f1c4-485e-be3f-131beddd2880",
   "metadata": {},
   "source": [
    "The corpora are divided into literary and non-literary text files. In the beginning, when I realized I could only download the files 1 by 1, I only downloaded literary texts from authors listed from A-B in alphabetical order. However, then I realized this would be a bad representation of the corpora as it wouldn't cover the same data. I think that literary texts are usually written in different language (structure, vocabulary, grammar) from non-literary texts, so I thought this wasn't a good representation of the Catalan Language as the corpora were intended to be. After thinking about this issue, I decided to download the same number of literary and non-literary works, as divided by the institution behind the corpora. Then I wanted to analyze and compare them to see how these might differ, and test my assumption that using only one type of those documents would have been a bad representation of Catalan language. Because of the downloading issue, I took only pieces of authors starting with the letter \"A\" in alphabetical order, hoping this smaller set of data, will still allow me to infer some ideas or points on ***how literary and non-literary texts in Catalan might be different in their basic statistics and language, and consequently, using only one of the two might lead to corpora biases.***\n",
    "\n",
    "**Some future wishes:**\\\n",
    "I was thinking about whether there's any way we could get only content words, as punctuation and function words didn't let me see the differences in content, that's why I tried to see them through hapaxes or extending the word n-grams to trigrams. Another thing I was hoping I could do but I couldn't is modify tokenization to fit my analysis. For example, I would have liked \"l'\" to be a single token, instead of the separate tokens \"l\" and \"'\", as intuitively as a Catalan speaker I would consider \"'\" in the same token as the apostrophized \"l.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62051c-f796-4189-8458-4117e8f66e57",
   "metadata": {},
   "source": [
    "## Analysing the corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426005e-719c-4206-bcf2-6902c453d74b",
   "metadata": {},
   "source": [
    "### Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b11c9149-7600-41ad-a61d-2136c51c164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARATION\n",
    "#Importing libraries and functions\n",
    "import nltk\n",
    "import os\n",
    "import numpy\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1b61034a-91cd-4472-8002-51ead9464427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPENING CORPORA\n",
    "corpus_root = 'data/'\n",
    "literary = os.path.join(corpus_root, 'literari')\n",
    "nonliterary = os.path.join(corpus_root, 'noliterari')\n",
    "\n",
    "lit = PlaintextCorpusReader(literary, r'.*\\.txt')\n",
    "nonlit = PlaintextCorpusReader(nonliterary, r'.*\\.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60406f6-7250-4c40-a07a-9c7da8c5567b",
   "metadata": {},
   "source": [
    "### Representative Sniplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9be45d9c-4421-47e0-bcd8-22105df95ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An example of part of one of the literary files is:\n",
      "\n",
      "Henoc és la ciutat mare  \n",
      "de les ciutats d'aquest món.  \n",
      "Viatger, ningú sap ara  \n",
      "les ruïnes a on són.\n",
      "\n",
      "A l'orient fou bastida  \n",
      "del carme isolat i buid  \n",
      "on penja l'inútil fruit  \n",
      "de l'arbre d'eterna vida.\n",
      "\n",
      "I dels vergers primitius,  \n",
      "a l'hora silenciosa,  \n",
      "creia sentir, enyorosa,  \n",
      "la remor dels quatre rius.\n",
      "\n",
      "II  \n",
      "Temps enrera, no hi havia  \n",
      "fites, partions ni tanques:  \n",
      "per tots, com la llum del dia,  \n",
      "eren els fruits de les branques.\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"An example of part of one of the literary files is:\")\n",
    "print()\n",
    "for line in lit.raw().splitlines()[10:30]: \n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d81ae5f8-8d26-4c18-aec5-9aa5074c140f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An example of part of one of the non-literary files is:\n",
      "\n",
      "\n",
      "Tothom pot patir dita malaltia, si bé l'edat en què és més freqüent és la de 40 a 65 anys.\n",
      "\n",
      "Totes les parts del cos poden ésser atacades, però les que ho són més sovint són: la matriu, l'estómac, la pell, la mamella, els llavis, la llengua, el budell, la gola, etz..\n",
      "\n",
      "El cranc no es traspassa de pares a fills, però les famílies en què hi han hagut crancs han de vigilar més encara que les altres.\n",
      "\n",
      "No coneixem la causa del cranc, però sabem que hi ha coses que favoreixen la seva aparició.\n",
      "\n",
      "Causes que favoreixen el cranc Tota irritació o inflamació continuades; el pes de la pipa sobre el llavi; les dentadures postisses mal ajustades, dents corcades que originen frecs o irritacions, les inflamacions i llagues de la matriu preparen el camí per què es presenti el cranc.\n",
      "\n",
      "En el pit de la dona, la inflamació o els cops a què està tan exposat; l'apretament de la cotilla o dels sostenidors, el frec continuat d'un elàstic, poden també ésser causes preparadores del cranc.\n",
      "\n",
      "Gran nombre de crancs de la pell són causats per berrugues insignificants, taques, etz., que han estat sotmeses a irritacions continuades.\n",
      "\n",
      "Les inflamacions cróniques de l'estómac, el restrenyiment habitual, la sífilis, l'abús de l'alcohol, de la cervesa, poden causar irritacions locals que favoreixen el cranc.\n",
      "\n",
      "Els obrers de certs oficis que presenten amb més freqüència el cranc: treballadors del quitrà, petroli, anilines, mariners, persones que treballen amb els raigs X, etz..\n",
      "\n",
      "Respecte a la predisposició al cranc per trastorns nutritius, res se sap de positiu: les estadístiques sols diuen que és més freqüent, en conjunt, en els rics que en els pobres i és un fet sabut la relació que existeix entre el cranc i la diabetes, tot això fa creure que es desenrotlla de preferència en els que fan excessos d'alimentació.\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"An example of part of one of the non-literary files is:\")\n",
    "print()\n",
    "for line in nonlit.raw().splitlines()[10:30]: \n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69f60e-18e4-4035-9970-e0c86274bfd0",
   "metadata": {},
   "source": [
    "### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6ac87a88-60a6-4757-8470-76586ad665ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26 literary texts in the corpora.\n",
      "There are 26 non-literary texts in the corpora.\n"
     ]
    }
   ],
   "source": [
    "#BASIC STATISTICS\n",
    "print(\"There are\", len(lit.fileids()), \"literary texts in the corpora.\")\n",
    "print(\"There are\", len(nonlit.fileids()), \"non-literary texts in the corpora.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "449a02a2-70ec-4e76-bcd4-32e0a8a98edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22081 sentences in the literary texts.\n",
      "There are 7050 sentences in the non-literary texts.\n",
      "The difference is: 15031\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(lit.sents()), \"sentences in the literary texts.\")\n",
    "print(\"There are\", len(nonlit.sents()), \"sentences in the non-literary texts.\")\n",
    "print(\"The difference is:\", len(lit.sents()) - len(nonlit.sents()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d06c728d-64f0-4eef-8ebf-e750a598f854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 534581 words in the literary texts.\n",
      "There are 198601 words in the non-literary texts.\n",
      "The difference is: 335980\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(lit.words()), \"words in the literary texts.\")\n",
    "print(\"There are\", len(nonlit.words()), \"words in the non-literary texts.\")\n",
    "print(\"The difference is:\", len(lit.words()) - len(nonlit.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257c710-58de-4272-9c0b-d84dd31990d8",
   "metadata": {},
   "source": [
    "**COMMENT:**\\\n",
    "Even though I kept the number of texts the same for literary and non-literary texts, both 26, we can see that the number of sentences and words differ considerably as we have about 15000 sentences more in literary texts than in non-literary texts. They also contrast remarkably in the number of words, having more than 30000 more words from literary texts than in non-literary texts. This could be an issue when contrasting the two types of texts in Catalan, as, while the number of documents are the same, the corpora are not even in size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d70a0-80d3-41e8-a43a-714ccbd28abb",
   "metadata": {},
   "source": [
    "### Building Data Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c0c1460e-228d-4efd-8eaa-3d247ac7cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building token lists\n",
    "lit_toks_upper = lit.words()\n",
    "lit_toks = [w.lower() for w in lit_toks_upper]\n",
    "nonlit_toks_upper = nonlit.words()\n",
    "nonlit_toks = [w.lower() for w in nonlit_toks_upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "81ddd7ff-b8fe-4009-8ad1-6804b612a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building word (unigram) frequency distributions\n",
    "lit_tokfd = nltk.FreqDist(lit_toks)\n",
    "nonlit_tokfd = nltk.FreqDist(nonlit_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "73d57a1f-9c56-445e-a79e-65ed62ffae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building word bigrams\n",
    "lit_bigrams = list(nltk.bigrams(lit_toks))\n",
    "nonlit_bigrams = list(nltk.bigrams(nonlit_toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f3a9ec70-5bc4-49af-bb31-6dd266050b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram frequency distributions\n",
    "lit_bigramfd = nltk.FreqDist(lit_bigrams)\n",
    "nonlit_bigramfd = nltk.FreqDist(nonlit_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1d7f0b9e-0af7-4991-97de-746981888283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conditional frequency distributions\n",
    "lit_bigramcfd = nltk.ConditionalFreqDist(lit_bigrams)\n",
    "nonlit_bigramcfd = nltk.ConditionalFreqDist(nonlit_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6f1bb804-e50e-49cc-a26a-a85fee59ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building word trigrams\n",
    "lit_trigrams = list(nltk.ngrams(lit_toks, 3))\n",
    "nonlit_trigrams = list(nltk.ngrams(nonlit_toks, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b749f3be-204b-4ca5-ac67-4308a6ced75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trigram frequency distributions\n",
    "lit_trigramfd = nltk.FreqDist(lit_trigrams)\n",
    "nonlit_trigramfd = nltk.FreqDist(nonlit_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd1020-3a90-4b56-a0ef-12434dc5f3c2",
   "metadata": {},
   "source": [
    "### Analyzing, Comparing and Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3f95f893-ef57-4465-915b-3567d97eae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of the literary texts is: 20560.808\n",
      "The average length of the non-literary texts is: 7638.5\n",
      "Difference: 12922.308\n"
     ]
    }
   ],
   "source": [
    "#comparing length in terms of words\n",
    "print(\"The average length of the literary texts is:\", round(len(lit.words())/len(lit.fileids()), 3))\n",
    "print(\"The average length of the non-literary texts is:\", round(len(nonlit.words())/len(nonlit.fileids()), 3))\n",
    "print(\"Difference:\", round(len(lit.words())/len(lit.fileids()) - len(nonlit.words())/len(nonlit.fileids()), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02e70b-7e27-439f-8f3a-bc9b1b5cbc8b",
   "metadata": {},
   "source": [
    "**Comment:**\\\n",
    "There is a huge difference in text-length, considering length as the number of words for each of the texts between literary and non-literary texts. The average length of the literary texts is almost 130000 words more than the average lengths of non-literary texts. That doesn't allow us to compare their content clearly, but is an important trend, or piece of data, that could be a criteria for distinguish between a literary and a non-literary text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1d57445e-c21e-48d1-92d5-9d70b8cae203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average sentence length in the literary texts is: 24.21\n",
      "The average sentence length in the non-literary texts is: 28.17\n"
     ]
    }
   ],
   "source": [
    "#comparing sentence length\n",
    "print(\"The average sentence length in the literary texts is:\", round(len(lit.words())/len(lit.sents()), 3))\n",
    "print(\"The average sentence length in the non-literary texts is:\", round(len(nonlit.words())/len(nonlit.sents()), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e8392f-d687-4901-bfe6-c7857e72720d",
   "metadata": {},
   "source": [
    "**Comment:**\\\n",
    "While we need to keep in mind that since the corpora we are comparing are very different in length, average sentence length seems to be longer for non-literary texts. As the average sentence length is about 3 words longer than that of literary texts. This seems like a reasonable conclusion as many of the non-literary texts are not made \"to enjoy\" but to be informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8b04bf02-15ee-4651-8541-e13e85893663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type-token ratio for the literary texts is: 0.413\n",
      "The type-token ratio for the non-literary texts is: 0.49\n"
     ]
    }
   ],
   "source": [
    "#type-token ratio\n",
    "print(\"The type-token ratio for the literary texts is:\", round(len(lit_bigramfd)/len(lit.words()), 3))\n",
    "print(\"The type-token ratio for the non-literary texts is:\", round(len(nonlit_bigramfd)/len(nonlit.words()), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ecb650-8da2-408a-99fa-eeffc48da2a6",
   "metadata": {},
   "source": [
    "**Comment:**\\\n",
    "Due to the great different in size between the two corpora, the TTR is not very relevant, and I believe is probably deceiving. The literary texts are much longer, and hence since type-token ratio does not increase linearly with text length, the type-token ratio of literary texts being still pretty close to that of non-literary texts despite text-length is probably showing us that literary texts have a more varied vocabulary. However, it is hard to tell because of the caveat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0d31a8e8-98a5-4698-85d5-a57eec39e3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 first hapaxes in the literary texts are: ['bíblics', 'unamuno', 'viatger', 'ruïnes', 'enyorosa', 'tanques', 'famílies', 'llepava', 'furts', 'nafrat']\n",
      "The 10 first hapaxes in the non-literary texts are: ['acadèmia', 'ciències', 'cat', '1924', 'emmetzinament', 'corrosiò', 'països', 'civilitzat', 'tisi', '700']\n"
     ]
    }
   ],
   "source": [
    "#hapaxes\n",
    "lit_hepx = [w for w in lit_tokfd.hapaxes()]\n",
    "nonlit_hepx = [w for w in nonlit_tokfd.hapaxes()]\n",
    "len(lit_hepx)\n",
    "len(nonlit_hepx)\n",
    "print(\"The 10 first hapaxes in the literary texts are:\", lit_hepx[0:10])\n",
    "print(\"The 10 first hapaxes in the non-literary texts are:\", nonlit_hepx[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b9cc06-6d90-4f65-bf9f-f1a3c5e9d4ec",
   "metadata": {},
   "source": [
    "**Comment:**\\\n",
    "Since as shown in the next analysis most words that most_common functions would allow us to see are function words or punctuation, I tried to look for differences in content in the hapaxes, as those were more likely to be content words. Yet, since they are words that only appear once in the corpora they are not descriptive of trends either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a21129a3-93f5-4cbe-a8e3-11797b07d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common unigrams in the literary texts on the corpus are:\n",
      "1 - (',', 34925)\n",
      "2 - (\"'\", 21206)\n",
      "3 - ('de', 17924)\n",
      "4 - ('que', 15710)\n",
      "5 - ('la', 15420)\n",
      "6 - ('.', 15108)\n",
      "7 - ('y', 14962)\n",
      "8 - ('lo', 7928)\n",
      "9 - ('no', 7347)\n",
      "10 - ('á', 7310)\n",
      "\n",
      "The most common unigrams in the non-literary texts on the corpus are:\n",
      "1 - (',', 12664)\n",
      "2 - ('de', 8240)\n",
      "3 - (\"'\", 7712)\n",
      "4 - ('.', 6557)\n",
      "5 - ('la', 6005)\n",
      "6 - ('que', 4798)\n",
      "7 - ('l', 3523)\n",
      "8 - ('y', 3512)\n",
      "9 - ('en', 3291)\n",
      "10 - ('d', 2688)\n",
      "\n",
      "The most common bigrams in the literary texts on the corpus are:\n",
      "1 - (('l', \"'\"), 5347)\n",
      "2 - ((',', 'y'), 4817)\n",
      "3 - (('d', \"'\"), 3871)\n",
      "4 - (('de', 'la'), 3304)\n",
      "5 - ((',', 'que'), 2413)\n",
      "6 - (('s', \"'\"), 2228)\n",
      "7 - (('qu', \"'\"), 2140)\n",
      "8 - (('.', '—'), 1473)\n",
      "9 - (('que', \"'\"), 1432)\n",
      "10 - (('á', 'la'), 1402)\n",
      "\n",
      "The most common bigrams in the non-literary texts on the corpus are:\n",
      "1 - (('l', \"'\"), 2982)\n",
      "2 - (('d', \"'\"), 2590)\n",
      "3 - (('de', 'la'), 1522)\n",
      "4 - ((',', 'y'), 752)\n",
      "5 - (('s', \"'\"), 673)\n",
      "6 - (('de', 'l'), 641)\n",
      "7 - (('de', 'les'), 524)\n",
      "8 - ((',', 'que'), 491)\n",
      "9 - (('de', 'las'), 398)\n",
      "10 - ((',', 'en'), 396)\n",
      "\n",
      "The most common trigrams in the literary texts on the corpus are:\n",
      "1 - (('de', 'l', \"'\"), 657)\n",
      "2 - (('que', \"'\", 's'), 422)\n",
      "3 - (('d', \"'\", 'un'), 416)\n",
      "4 - (('que', 's', \"'\"), 380)\n",
      "5 - ((',', 'qu', \"'\"), 361)\n",
      "6 - ((',', 'l', \"'\"), 351)\n",
      "7 - (('qu', \"'\", 'es'), 341)\n",
      "8 - (('que', 'l', \"'\"), 334)\n",
      "9 - (('á', 'l', \"'\"), 305)\n",
      "10 - (('d', \"'\", 'una'), 299)\n",
      "\n",
      "The most common trigrams in the non-literary texts on the corpus are:\n",
      "0 - (('de', 'l', \"'\"), 636)\n",
      "1 - (('a', 'l', \"'\"), 239)\n",
      "2 - ((',', 'l', \"'\"), 236)\n",
      "3 - (('d', \"'\", 'un'), 227)\n",
      "4 - (('d', \"'\", 'una'), 225)\n",
      "5 - (('en', 'l', \"'\"), 177)\n",
      "6 - (('.', 'l', \"'\"), 137)\n",
      "7 - (('que', 'l', \"'\"), 136)\n",
      "8 - (('(', 'vid', '.'), 128)\n",
      "9 - (('vid', '.', 'secc'), 125)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#unigram frequencies\n",
    "top10_lituni = enumerate(lit_tokfd.most_common(10))\n",
    "print(\"The most common unigrams in the literary texts on the corpus are:\")\n",
    "for element in top10_lituni:\n",
    "    print(element[0]+1, \"-\", element[1])\n",
    "print()\n",
    "\n",
    "top10_nonlituni = enumerate(nonlit_tokfd.most_common(10))\n",
    "print(\"The most common unigrams in the non-literary texts on the corpus are:\")\n",
    "for element in top10_nonlituni:\n",
    "    print(element[0]+1, \"-\", element[1])\n",
    "print()\n",
    "\n",
    "#bigram frequencies\n",
    "top10_litbi = enumerate(lit_bigramfd.most_common(10))\n",
    "print(\"The most common bigrams in the literary texts on the corpus are:\")\n",
    "for element in top10_litbi:\n",
    "    print(element[0]+1, \"-\", element[1])\n",
    "print()\n",
    "\n",
    "top10_nonlitbi = enumerate(nonlit_bigramfd.most_common(10))\n",
    "print(\"The most common bigrams in the non-literary texts on the corpus are:\")\n",
    "for element in top10_nonlitbi:\n",
    "    print(element[0]+1, \"-\", element[1])\n",
    "print()\n",
    "\n",
    "#trigram frequencies\n",
    "top10_littri = enumerate(lit_trigramfd.most_common(10))\n",
    "print(\"The most common trigrams in the literary texts on the corpus are:\")\n",
    "for element in top10_littri:\n",
    "    print(element[0]+1, \"-\", element[1])\n",
    "print()\n",
    "\n",
    "top10_nonlittri = enumerate(nonlit_trigramfd.most_common(10))\n",
    "print(\"The most common trigrams in the non-literary texts on the corpus are:\")\n",
    "for element in top10_nonlittri:\n",
    "    print(element[0], \"-\", element[1])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd74899-bc18-465d-8870-4eb75d460edf",
   "metadata": {},
   "source": [
    "**Comment:** The 10 most common ungirams, trigrams and bigrams in the literary and non-literary texts are all function words and punctuation marks. This is not very informative on any differences between literary and non-literary texts, as these tokens are frequent in any text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b039ec46-c1a3-4b73-b1b4-722ef8547425",
   "metadata": {},
   "source": [
    "### Finding distinctive features using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7472cf82-2f93-4cbe-942e-7d13698f59fe",
   "metadata": {},
   "source": [
    "As most attempts to find relevant differences, except the text-length point, were pretty decieving, as most of my approches only gave me function words and punctuation that are no very helpful in distinguishing between types of texts, I tried to have Native Bayes find distinctive features for me.\\\n",
    "In the following code I labeled the sentences in literary and nonliterary, shuffled them and divided them for training, testing and a development-test set. After that, I used the gen_feats function writen by Na-Rae Han in one of the homeworks for LING 1330 to create a features dictionary with the words that each sentence contains. Once done, I trained the Classifier named as \"docClass.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "89a63e5d-0d85-4585-a36b-b40bd000f721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 9000\n",
      "Number of development-test sentences: 10000\n",
      "Number of training sentences: 10131\n"
     ]
    }
   ],
   "source": [
    "lit_sents = lit.sents()\n",
    "nonlit_sents = nonlit.sents()\n",
    "lablit_sents = [(s, \"literary\") for s in lit_sents]\n",
    "labnonlit_sents = [(s, \"nonliterary\") for s in nonlit_sents]\n",
    "sents = lablit_sents+labnonlit_sents\n",
    "\n",
    "import random\n",
    "random.Random(10).shuffle(sents)\n",
    "\n",
    "len(sents)/3\n",
    "\n",
    "test_sents = sents[:9000]\n",
    "devtest_sents = sents[9000:19000]\n",
    "train_sents = sents[19000:]\n",
    "\n",
    "print(\"Number of test sentences:\", len(test_sents))\n",
    "print(\"Number of development-test sentences:\", len(devtest_sents))\n",
    "print(\"Number of training sentences:\", len(train_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "098cfd92-0fa3-4c1d-8eab-dcd7c765f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_feats(sent):\n",
    "    featdict = {}\n",
    "    for w in sent:\n",
    "        featdict['contains-'+w.lower()] = 1\n",
    "    return featdict\n",
    "test_feats = [(gen_feats(sent), doctype) for sent, doctype in test_sents]\n",
    "devtest_feats = [(gen_feats(sent), doctype) for sent, doctype in devtest_sents]\n",
    "train_feats = [(gen_feats(sent), doctype) for sent, doctype in train_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c72af08d-f48b-482f-8e9d-02821de3d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_class = nltk.NaiveBayesClassifier.train(train_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675c907-8b6d-44c0-a68e-dea7c547a74b",
   "metadata": {},
   "source": [
    "### Putting the classifier to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "55588c50-d470-446f-8cd7-852e590a0add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier in distinguishing between literary and non-literary texts is:\n",
      "0.845\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.accuracy(doc_class, test_feats)\n",
    "print(\"The accuracy of the classifier in distinguishing between literary and non-literary texts is:\")\n",
    "print(round(accuracy, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5291d1-616c-4d61-b146-485cced61d51",
   "metadata": {},
   "source": [
    "***Comment:*** The accuracy of this classifier is lower than I expected it to be, which might mean that literary and non-literary texts are not so different or easly distinguishable. Since Naive Bayes can't correctly label them as one text type or the other that it does it more than 85% of the time. It is close to that, but I honestly expected it to be higher.\\\n",
    "To see why the classifier was having problems I created 4 arrays one for each possibility answering the question is it a literary test?\\\n",
    "(ll) true positive, (nn) true negative, (ln) false negative, (nl) false positive.\\\n",
    "After that I randomly observed a sentence from each of those arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "70fb8f26-6c17-406f-a4a6-b5f9b7bbd4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ll: text is literary, guessed literary\n",
    "#nn: text is non-literary, guessed non-literary\n",
    "#ln: text is literary, guessed non-literary\n",
    "#nl: text is non-literary, guessed literary\n",
    "ll, nn, ln, nl = [], [], [], []\n",
    "for (sent, doctype) in devtest_sents:\n",
    "    guess = doc_class.classify(gen_feats(sent))\n",
    "    if doctype == \"literary\" and guess == \"literary\":\n",
    "        ll.append((doctype, guess, sent))\n",
    "    elif doctype == \"nonliterary\" and guess == \"nonliterary\":\n",
    "        nn.append((doctype, guess, sent))\n",
    "    elif doctype == \"literary\" and guess == \"nonliterary\":\n",
    "        ln.append((doctype, guess, sent))\n",
    "    elif doctype == \"nonliterary\" and guess == \"literary\":\n",
    "        nl.append((doctype, guess, sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "64aeaa92-c8d5-4fb8-8df7-deee8f843c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL=literary GUESS=literary\n",
      "Los de Girona , que ja estavan escarmentats , perque sempre havian anat de nassos per terra , mentres passávam ' ms deyan : \" anéu , anéu en nom de Dèu , ja veureu lo pá que hi dònan .\"\n",
      "-------\n",
      "REAL=nonliterary GUESS=nonliterary\n",
      "En vista de tot aixó , la qüestió queda reduhida á que se ompli la suscripció de accions que se obrirá , del valor y condicions indicadas ; y á que las adhesions que se rebin dels senyors terratinents , representin las duas terceras parts dels terrenos regables ; perque aixó significaría á la vegada , la necessaria confiansa en lo negoci , y la disposició consegüent , no menos necessaria , pera portarlo avant .\n",
      "-------\n",
      "REAL=literary GUESS=nonliterary\n",
      "Després dels amors de Gretchen , qui iniciaven tota una era de l ' amor humà , se sentia atret per la bellesa sexual d ' Helena i aprenia en sos braços el secret de les civilisacions extingides .\n",
      "-------\n",
      "REAL=nonliterary GUESS=literary\n",
      "95 .\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for x in (ll, nn, ln, nl):\n",
    "    doctype, guess, sent = random.choice(x)\n",
    "    print('REAL=%-8s GUESS=%-8s' % (doctype, guess))  # string formatting\n",
    "    print(' '.join(sent))\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d42a1-883c-4dd1-b929-cbd04063e34e",
   "metadata": {},
   "source": [
    "The **first sentence (ll)** clearly belongs to a literary text, as it is correctly labeled. We can spot it as such because it includes a direct speech in quotes \"\", and expressive figurative elements such as \"de nassos per terra,\" which means to fall but in a descriptive way, as the literal translation would be \"to fall on your nose.\" We also find colloquial expressions such as \"ja veureu lo pá que hi dònaran.\"\\\n",
    "\n",
    "The **second sentence (nn)** is clearly non-literary because it discusses values, selling, and buying fields. It also contains words such as \"negoci\" meaning \"business,\" which might tell the classifier that this is not a literary text.\\\n",
    "\n",
    "The **third sentence (ln)** surprises me as a false negative, as it is narrating the story of a character and is easly spoted as literary text from a content/human reading perspective. Yet, the classifier missed it.\\\n",
    "\n",
    "The **fourth \"sentence\" (nl)** is just a number, which suggests to me that I should probably do some cleaning of the data and repeat the process, as there is no way, the classifier could decide wether a \"95\" belongs to a literary or a non-literary text, and this might be making its accuracy drop significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "32750678-e3f2-4aec-a03c-3253134a8851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "      contains-filosofia = 1              nonlit : litera =     57.3 : 1.0\n",
      "           contains-idem = 1              nonlit : litera =     55.3 : 1.0\n",
      "   contains-combinacions = 1              nonlit : litera =     51.1 : 1.0\n",
      "          contains-idees = 1              nonlit : litera =     42.7 : 1.0\n",
      "        contains-nocions = 1              nonlit : litera =     40.7 : 1.0\n",
      "         contains-servei = 1              nonlit : litera =     34.4 : 1.0\n",
      "          contains-poeta = 1              nonlit : litera =     29.9 : 1.0\n",
      "         contains-causes = 1              nonlit : litera =     28.1 : 1.0\n",
      "    contains-experiencia = 1              nonlit : litera =     28.1 : 1.0\n",
      "     contains-literatura = 1              nonlit : litera =     26.9 : 1.0\n",
      "         contains-castes = 1              nonlit : litera =     26.1 : 1.0\n",
      "           contains-onas = 1              nonlit : litera =     24.7 : 1.0\n",
      "       contains-agregats = 1              nonlit : litera =     24.0 : 1.0\n",
      "          contains-lleis = 1              nonlit : litera =     24.0 : 1.0\n",
      "       contains-periodes = 1              nonlit : litera =     24.0 : 1.0\n",
      "         contains-secció = 1              nonlit : litera =     24.0 : 1.0\n",
      "       contains-aquestes = 1              nonlit : litera =     23.7 : 1.0\n",
      "     contains-composició = 1              nonlit : litera =     21.9 : 1.0\n",
      "         contains-conreu = 1              nonlit : litera =     21.9 : 1.0\n",
      "     contains-diccionari = 1              nonlit : litera =     21.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "doc_class.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fcd77c-db50-4aaf-bb4d-f227be42bf91",
   "metadata": {},
   "source": [
    "Despite the issue with the data not being properly cleaned, and the sentences not being all sentences (numbers like the misslabeled 95) that probably decrease the accuracy significantly, the classifier does a better job at finding meaningful differences between literary and non-literary files than what we could see with the n-grams.\\\n",
    "We can see that the words \"lleis\" in English \"laws\" and \"causes\" in English also \"causes\" are two words that are shown as informative features that make the classifier tilt more towards labeling a file as non-literary, which is very intuitive. We also see that it seems like non-literary texts contain more distinctive features that flag them as non-literary than literary texts, as we don't see any words being valuable for that decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb25a483-8d83-4d54-8da3-ea9b08f7e6af",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea0cb76-e053-4414-a4c5-1ef2963d94bf",
   "metadata": {},
   "source": [
    "I'd like to re-do this process with proper data cleaning even though I don't have these skills at the moment, so this could be another point included in my **future wishes section**. I really think that with proper data cleaning, the accuracy of the classifier would improve significantly.\n",
    "\n",
    "Still, looking at this analysis and working with what we have, it seems to me that there are enough significant features in non-literary texts, flagging them as non-literary, that restricting corpora to only non-literary texts would not represent the true distribution of the Catalan language. I think it would create corpus biases. As for example, \"shares\" is more commonly tagged as a noun than a verb in the Penn Treebank, which does not match most speakers' intuitions but happens because of its source; The Wall Street Journal. Instead, as there seem to be fewer distinctive features for literary texts, it seems to me that we could probably use literary texts as more representative corpora of the Catalan language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
